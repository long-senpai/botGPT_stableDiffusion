AI integration on Teiki\n\nIntroduction\nThe co-evolution of humans and machines has been central to social advancement throughout history. With the rapid growth of AI, we have seen a clear trend in recent times. Everyone uses Grammarly. Midjourney was released only six months ago. ChatGPT took over the world in a matter of days.\n\nAI is reducing friction for millions of technology creators and users. Grammarly reduces the editing time from days to minutes so humans can focus more on the written content. Working together, Midjourney saves time for human artists to focus on more essential drawings. A novelist can complete a light novel without working with non-dedicated artists. Creators can create a banner without the need to brief a human graphic designer. The list goes on.\n\nCatching on to this trendy user experience, we integrate numerous AI to aid Teiki creators and backers. Our content moderation AI removes the need for a long review process. The generative models and writing assistance help creators start a project in days. The process usually takes weeks on platforms like Kickstarter. KYC alone needs 3–7 days. For backers, Teiki summarizes and recommends content to improve the browsing experience. We also have podcast generation, OCR, and much more in the future.\n\nThese AI integrations maximize UX, minimize the time one needs to raise funding, and cut human costs. Combined with Cardano’s low transaction fees, Teiki offers a crowdfunding solution that is several times cheaper and lower in friction than alternatives.\n\nContent Moderation\n\nAs a decentralized protocol, Teiki avoids censorship as much as possible. Nevertheless, unmoderated fundraising and carelessly serving unmoderated content are potentially harmful. In the first generation, Teiki integrates content moderation AI to detect inappropriate projects (for instance, raising money for political activities) and warn them from posting. Violated projects will be delisted, and fined their pledge. The procedure allows us to skip manual reviews for projects, which is a massive friction in traditional platforms. The system also flags inappropriate content in the user interface that needs an extra click to show. We plan to further decentralize the moderation system in the second generation by making more decisions via on-chain governance.\n\nWe rely on transformer-based models such as Google’s BERT[1] for natural language processing. The model takes text inputs of under 2000 words and categorizes them into “toxic”, “severe toxic”, “threat”, “insult”, “obscene”, “identity hate”, “political”, and “discrimination”. We further use NudeNet[2] to detect sexually explicit images. Current QC shows excellent results on everything but drugs, discrimination, and manually censored obscenity. We also need to run several models to cover the many cases. We plan to design a single NLP model to reduce inference time, support audio moderation, and add hierarchical algorithms to handle manual obscuration.\n\nKeyword Extraction\nWe use keyword extraction to identify common tags from a project description. The tags are helpful for generative models, as a “technology” project would have a different illustration style from a “creative” one. For the backers, these tags allow them to search and filter projects more effectively.\n\nWe currently use YAKE[3], a lightweight unsupervised keyword extraction method that rests on statistical text features from single documents. We do not need to train the model on a particular set of documents, but it depends on dictionaries, external corpus, text size, language, and domain. In the future, we plan to add part-of-speech tagging to the pre-processing and more autonomous post-processing.\n\nName Generation\nNaming a project takes work. Specifically, the naming process must compress relevant information and a creative concept into a single signature representing the mission, core values, and more. Hence, we propose two autonomous methods: a naive approach and a more advanced one with deep learning. The naive approach determines the synonyms of the extracted keyword and translates those into other languages for uniqueness. The more advanced method employs a text-generation recurrent neural network. From the project description, the AI model generates a list of names using both methods for the creator to select and modify quickly.\n\nWe will adapt the deep learning model to our larger crowdfunding dataset for future improvements and work with the community for more creative naming approaches.\n\nTagline Generation\nA tagline is a catchy and declarative sentence that encapsulates a project. Coming up with one is a creative process that is potentially frictional and time-consuming. We use AI to generate multiple options based on the project description. Creators can then select or modify one to quickly have a tagline.\n\nIn some aspects, a tagline is comparable to an abstract summary with a text generation structure and the same input and output format. The main disparity between these two is their compactness, objectiveness in meaning, and semantic features. Therefore, we use the text summarization approach followed by a text generation deep learning model. We choose GPT2[4], which produces decent results while scales much better than GPT3[5] for the use case. We have crawled nearly 10,000 slogans from top-tier companies and retrained GPT2 on the dataset. In the future, we will continue to train the model on more extensive data from our crowdfunding data pipeline.\n\nLogo & Illustration Generation\n\nCreating graphic materials is a very time-consuming process. It may take several days for a designer to understand the requirements and mood of a new logo. The cost becomes significant when the exchanges and modifications drag on. Generative models are compelling in removing this human friction, especially for projects that do not need a shiny logo, like an educational course or open-source software. Having Teiki generate the logo and community update banners allow these projects to focus on their expertise.\n\nOur AI models synthesize logos and illustrations based on the project name, tagline, and keywords. The image generation system consists of a text encoder, an image information creator, and an image decoder. We currently employ a fine-tuned latent diffusion model to generate letter mark logos. We use Stable Diffusion[6] for illustration generation with a fine-tuned prompt from the project tagline.\n\nWhile Stable Diffusion results are significant, we still need to improve the logos in the future. The plan is to retrain a generative model on crowdfunding and popular logos and to support more logotypes.\n\nText Summarization\nUsers have built a tendency to fast scroll through content on news and social applications. It is unrealistic to expect Teiki users read through endless paragraphs introducing each project and update. It would be better to utilize summarisation models that trim content to fit a typical Mobile screen. We start with text summarisation and will employ one-pager generation in the future.\n\nFor text summarization, we have tried both extractive and abstractive methods. As for the extractive approach, these models could easily focus on a redundant marketing remark or off-the-subject information, leading to useless details in the output. The output summary also lingers between indicative and informative types, making it generally hard to read. Therefore, we decided to follow the abstractive approach, precisely, a pre-trained BART[7] model from Hugging Face that yields the best results in our QC. A future improvement is to retrain the model on our crowdfunding data.\n\nText Recognition\nMany projects upload information as decorated images for visual branding. Optical character recognition (OCR) identifies text from these images for analysis and search. It can extract keywords from the images to suggest additional content like taglines and a summary. Scanned text is run through moderation to ensure offensive content is not embedded. On the other hand, backers can search for these projects with text in the images. The key is to enable visually engaging content while not sacrificing the powerful text representation. We currently use PaddleOCR[8] for ease of integration.\n\nSentiment Analysis\nWe run sentiment and emotion analyses on user writings to flag inappropriate content. Furthermore, it helps creators write more engaging community updates and ensure that their writings are not offensive. Combined with the Grammarly integration, users can write effectively and confidently. The goal is to let users spend more time building instead of writing.\n\nAfter several experiments, we chose two fine-tuned BERT models in Twitter-roBERTa-base[9] and emoroBERTa[10]. The first model takes an input of under 2000 words to see if it is negative, neutral, or positive. If the output is highly negative or positive, the second model extracts up to 26 emotions for further analysis and suggestions. We do section analysis to show users as close as possible where things are off.\n\nIn the future, we plan to design our model to output only the relevant emotions and improve inference time.\n\nRecommendation System\nTeiki recommends projects a user may like based on their behaviors. The protocol indexer tracks projects with tags for the recommendation system to infer. We currently use a popular algorithm with the tf–idf representation, where the system creates content-based user profiles from a weighted vector of item features. The weights denote the importance of each feature to the user. They can be computed from individually rated content vectors using various techniques, from average values to probability-based machine learning.\n\nWe currently recommend based on which project tags a user is backing. In the future, we will integrate Discord login for users to follow projects manually as another factor. Finally, we will add past interactions to the formula, a better feature representation, and a more sophisticated algorithm.\n\nSpeech Synthesis\n\nIt is easier to consume audio than text for many people. Therefore, we build a single-speaker Text-to-Speech (TTS) model to read project details aloud. We go further by combining these speeches into podcasts for users to catch up with project updates while on a train, driving to work, or in a shower.\n\nWe adopt an end-to-end neural network-based TTS approach as the primary solution as it requires less feature development and can produce high voice quality. We use a pre-trained VITS[11] model as the baseline to develop our TTS system. VITS novelly presents waveforms by modeling a variational autoencoder with a flow-based decoder. Also, It utilizes MAS[12] to learn a text-to-audio alignment and therefore does not need annotations of the external data.\n\nIn the future, we plan to tune the model to the parrot voice of Niko, the mascot of Teiki, to improve immersion.\n\nConclusion\nTeiki integrates numerous AI to reduce friction for creators and backers. The goal is to maximize UX, minimize the time one needs to raise funding, and cut operational costs. The AI models also constantly learn with user data and decisions, making Teiki a decentralized crowdfunding platform where different agents can co-evolve.\n\nWe will cover more technical details in a January white paper and testnet launch. And open-source the AI code in Q2 2023. Stay tuned!